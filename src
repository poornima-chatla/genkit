import * as z from 'zod';

// Import the Genkit core libraries and plugins.
import { generate, index } from '@genkit-ai/ai';
import { configureGenkit } from '@genkit-ai/core';
import { defineFlow, runFlow, startFlowsServer, streamFlow } from '@genkit-ai/flow';
import { googleAI } from '@genkit-ai/googleai';

// Import models from the Google AI plugin. The Google AI API provides access to
// several generative models. Here, we import Gemini 1.5 Flash.
import { gemini15Flash } from '@genkit-ai/googleai';
import { getFirestoreDocumentRetriever, textEmbeddingGecko, vertexAI, vertexAiIndexerRef, vertexAiRetrieverRef } from '@genkit-ai/vertexai';
import { embed } from '@genkit-ai/ai/embedder';
import { Document, retrieve } from '@genkit-ai/ai/retriever';

configureGenkit({
  plugins: [
    // Load the Google AI plugin. You can optionally specify your API key
    // by passing in a config object; if you don't, the Google AI plugin uses
    // the value from the GOOGLE_GENAI_API_KEY environment variable, which is
    // the recommended practice.
    googleAI(),
  ],
  // Log debug output to tbe console.
  logLevel: 'debug',
  // Perform OpenTelemetry instrumentation and enable trace collection.
  enableTracingAndMetrics: true,
});

// Define a simple flow that prompts an LLM to generate menu suggestions.
export const menuSuggestionFlow = defineFlow(
  {
    name: 'menuSuggestionFlow',
    inputSchema: z.string(),
    outputSchema: z.string(),
  },
  async (subject) => {
    // Construct a request and send it to the model API.
    const llmResponse = await generate({
      prompt: `Suggest an item for the menu of a ${subject} themed restaurant`,
      model: gemini15Flash,
      config: {
        temperature: 1,
      },
    });

    // Handle the response from the model API. In this sample, we just convert
    // it to a string, but more complicated flows might coerce the response into
    // structured output or chain the response into another LLM call, etc.
    return llmResponse.text();
  }
);

export const summarizeTextFlow = defineFlow({
  name: 'summarizeTextFlow',
  inputSchema: z.string(),
  outputSchema: z.string(),
},
  async (text) => {
    const prompt = `Please answer all questions truthfully. If you do not know the answer say "I do not know". Please summarize the text for me: ${text}`;

    const llmResponse = await generate({
      model: gemini15Flash,
      prompt,
    });
    return llmResponse.text();
  }
)

export const articlesIndexer = vertexAiIndexerRef({ indexId: 'articles' });
export const articlesRetriever = vertexAiRetrieverRef({ indexId: 'articles' });


export const semanticSearch = defineFlow(
  {
    name: 'semanticSearchFlow',
    inputSchema: z.string(),
    outputSchema: z.string(),
  },
  async (query) => {
    const docs = await retrieve({
      retriever: articlesRetriever,
      query,
      options: { k: 3 },
    });

    const context = docs.map((doc) => doc.text()).join('\n\n');
    const prompt = `use the following pieces of context to answer the question at the end. If you do not know the answer, just say i do not know
                   ${context}
                   Question: ${query}
                   Helpful answer: ` ;

    const answer = await generate({ model: gemini15Flash, prompt });
    return answer.text();
  }
)


// Start a flow server, which exposes your flows as HTTP endpoints. This call
// must come last, after all of your plug-in configuration and flow definitions.
// You can optionally specify a subset of flows to serve, and configure some
// HTTP server options, but by default, the flow server serves all defined flows.
startFlowsServer();
